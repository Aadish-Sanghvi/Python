{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXkRvvpkvj-t"
      },
      "source": [
        "## Step 1 – Standardize Your Data Import Process\n",
        "I built a function that loads data consistently, no matter the format. This tiny change made my workflow so much smoother. This function ensures that no matter what format your data comes in, it will be loaded consistently, reducing manual adjustments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ac1OB3vRiY8z"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, **kwargs):\n",
        "    \"\"\"\n",
        "    Load data from various file formats while handling common issues.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    from pathlib import Path\n",
        "    \n",
        "    file_type = Path(file_path).suffix.lower()\n",
        "    \n",
        "    handlers = {\n",
        "        '.csv': pd.read_csv,\n",
        "        '.xlsx': pd.read_excel,\n",
        "        '.json': pd.read_json,\n",
        "        '.parquet': pd.read_parquet\n",
        "    }\n",
        "    \n",
        "    reader = handlers.get(file_type)\n",
        "    if reader is None:\n",
        "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
        "    \n",
        "    df = reader(file_path, **kwargs)\n",
        "    df.columns = df.columns.str.strip().str.lower()  # Standardize column names\n",
        "    df = df.replace('', pd.NA)  # Convert empty strings to NA\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXyrWXTGvqkY"
      },
      "source": [
        "## Step 2 – Implement Automated Data Validation\n",
        "Instead of manually checking for issues, automating the validation process ensures cleaner, more reliable data without the hassle.\n",
        "\n",
        "First, we define the validation rules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EyVN1pbwjz22"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def validate_dataset(df, validation_rules=None):\n",
        "    \"\"\"\n",
        "    Apply validation rules to a dataframe and return validation results.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        validation_rules (dict): Dictionary of column names and their validation rules\n",
        "        \n",
        "    Returns:\n",
        "        dict: Validation results with issues found\n",
        "    \"\"\"\n",
        "    if validation_rules is None:\n",
        "        validation_rules = {\n",
        "            'numeric_columns': {\n",
        "                'check_type': 'numeric',\n",
        "                'min_value': 0,\n",
        "                'max_value': 1000000\n",
        "            },\n",
        "            'date_columns': {\n",
        "                'check_type': 'date',\n",
        "                'min_date': '2000-01-01',\n",
        "                'max_date': '2025-12-31'\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    # We then apply the checks and return the results:\n",
        "    validation_results = {}\n",
        "    \n",
        "    for column, rules in validation_rules.items():\n",
        "        if column not in df.columns:\n",
        "            continue\n",
        "            \n",
        "        issues = []\n",
        "        \n",
        "        # Check for missing values\n",
        "        missing_count = df[column].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            issues.append(f\"Found {missing_count} missing values\")\n",
        "            \n",
        "        # Type-specific validations\n",
        "        if rules['check_type'] == 'numeric':\n",
        "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "                issues.append(\"Column should be numeric\")\n",
        "            else:\n",
        "                out_of_range = df[\n",
        "                    (df[column] < rules['min_value']) | \n",
        "                    (df[column] > rules['max_value'])\n",
        "                ]\n",
        "                if len(out_of_range) > 0:\n",
        "                    issues.append(f\"Found {len(out_of_range)} values outside allowed range\")\n",
        "\n",
        "        elif rules['check_type'] == 'date':\n",
        "            if not pd.api.types.is_datetime64_any_dtype(df[column]):\n",
        "                issues.append(\"Column should be a date\")\n",
        "            else:\n",
        "                df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "                out_of_range = df[\n",
        "                    (df[column] < pd.to_datetime(rules['min_date'])) | \n",
        "                    (df[column] > pd.to_datetime(rules['max_date']))\n",
        "                ]\n",
        "                if len(out_of_range) > 0:\n",
        "                    issues.append(f\"Found {len(out_of_range)} dates outside allowed range\")\n",
        "                    \n",
        "        validation_results[column] = issues\n",
        "    \n",
        "    return validation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Sbs42VwMdi"
      },
      "source": [
        "## Step 3 – Create a Data Cleaning Pipeline\n",
        "I’ve worked with datasets full of duplicates, inconsistent formats, and missing values, and trust me — manually fixing these issues is painful.\n",
        "\n",
        "That’s why I built a modular data-cleaning pipeline.\n",
        "\n",
        "Instead of writing scattered cleaning scripts, this approach lets you structure everything in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W4lCuzTJkVRb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    \"\"\"\n",
        "    A modular pipeline for cleaning data with customizable steps.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.steps = []\n",
        "        \n",
        "    def add_step(self, name, function):\n",
        "        \"\"\"Add a cleaning step.\"\"\"\n",
        "        self.steps.append({'name': name, 'function': function})\n",
        "        \n",
        "    def execute(self, df):\n",
        "        \"\"\"Execute all cleaning steps in order.\"\"\"\n",
        "        results = []\n",
        "        current_df = df.copy()\n",
        "        \n",
        "        for step in self.steps:\n",
        "            try:\n",
        "                current_df = step['function'](current_df)\n",
        "                results.append({\n",
        "                    'step': step['name'],\n",
        "                    'status': 'success',\n",
        "                    'rows_affected': len(current_df)\n",
        "                })\n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    'step': step['name'],\n",
        "                    'status': 'failed',\n",
        "                    'error': str(e)\n",
        "                })\n",
        "                break\n",
        "                \n",
        "        return current_df, results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyylQ0h1v2Ap"
      },
      "source": [
        "## Step 4 – You can then define functions to add data-cleaning steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Hic0lH3pkaYy"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(df):\n",
        "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    outliers_removed = {}\n",
        "\n",
        "    for column in numeric_columns:\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Count outliers before removing\n",
        "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].shape[0]\n",
        "\n",
        "        # Cap the values instead of removing them\n",
        "        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "        if outliers > 0:\n",
        "            outliers_removed[column] = outliers\n",
        "\n",
        "    return df, outliers_removed\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    return df.drop_duplicates()\n",
        "\n",
        "def standardize_dates(df):\n",
        "    date_columns = df.select_dtypes(include=['datetime64']).columns\n",
        "    for col in date_columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "    return df\n",
        "\n",
        "def clean_text_columns(df, columns=None):\n",
        "    \"\"\"\n",
        "    Apply standardized text cleaning to specified columns.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        columns (list): List of columns to clean. If None, clean all object columns\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe with cleaned text columns\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = df.select_dtypes(include=['object']).columns\n",
        "        \n",
        "    df = df.copy()\n",
        "    \n",
        "    for column in columns:\n",
        "        if column not in df.columns:\n",
        "            continue\n",
        "            \n",
        "        # Apply string cleaning operations\n",
        "        df[column] = (df[column]\n",
        "                     .astype(str)\n",
        "                     .str.strip()\n",
        "                     .str.lower()\n",
        "                     .replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces\n",
        "                     .replace(r'[^\\w\\s]', '', regex=True))  # Remove special characters\n",
        "                     \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the pipeline like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = DataCleaningPipeline()\n",
        "pipeline.add_step('remove_duplicates', remove_duplicates)\n",
        "pipeline.add_step('remove_outliers', remove_outliers)\n",
        "pipeline.add_step('standardize_dates', standardize_dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCV0vKBcwVVB"
      },
      "source": [
        "## Step 5 – Monitor Data Quality Over Time\n",
        "\n",
        "Keeping track of data quality helps catch these issues early.\n",
        "\n",
        "Instead of waiting for problems to surface in reports or dashboards, I use an automated monitoring function to track key metrics and compare them against previous baselines.\n",
        "\n",
        "The monitoring function below helps you track key quality metrics and identify potential issues before they become problems:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5mCT72R8ke2r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_quality_metrics(df, baseline_metrics=None):\n",
        "    \"\"\"\n",
        "    Generate quality metrics for a dataset and compare with baseline if provided.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        baseline_metrics (dict): Previous metrics to compare against\n",
        "        \n",
        "    Returns:\n",
        "        dict: Current metrics and comparison with baseline\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'row_count': len(df),\n",
        "        'missing_values': df.isna().sum().to_dict(),\n",
        "        'unique_values': df.nunique().to_dict(),\n",
        "        'data_types': df.dtypes.astype(str).to_dict()\n",
        "    }\n",
        "    \n",
        "    # Add descriptive statistics for numeric columns\n",
        "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
        "    metrics['numeric_stats'] = df[numeric_columns].describe().to_dict()\n",
        "    \n",
        "    # Compare with baseline if provided\n",
        "    if baseline_metrics:\n",
        "        metrics['changes'] = {\n",
        "            'row_count_change': metrics['row_count'] - baseline_metrics['row_count'],\n",
        "            'missing_values_change': {\n",
        "                col: metrics['missing_values'][col] - baseline_metrics['missing_values'].get(col, 0)\n",
        "                for col in metrics['missing_values']\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    return metrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
